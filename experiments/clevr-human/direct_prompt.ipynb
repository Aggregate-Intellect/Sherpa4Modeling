{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from datasets import load_dataset\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "import re\n",
    "import json\n",
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"gpt-4o-mini\"\n",
    "llm = ChatOpenAI(model=model_name, temperature=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"Dogdays/clevr_subset\", token=True)[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_TEMPLATE = \"\"\"You are a careful assistant helping a visually impaired person to answer questions regarding a scene. The scene is described in JSON format.\n",
    "\n",
    "Scene: {scene}\n",
    "\n",
    "First examine the scene and then check the following question. Reason about how this question can be answered before provide the final answer. The answer should either be a number, yes or no or an attribute value.\n",
    "\n",
    "Give the final answer in the following JSON format:\n",
    "```json\n",
    "{{\n",
    "    \"answer\": \"<answer to the question>\"\n",
    "}}\n",
    "```\n",
    "\n",
    "Question: {question}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = PromptTemplate.from_template(PROMPT_TEMPLATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_json(text):\n",
    "    json_pattern = re.compile(r\"```json\\n((.|\\n)*?)\\n```\")\n",
    "\n",
    "    match = json_pattern.search(text)\n",
    "    if match:\n",
    "        return json.loads(match.group(1))\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90a086e4f5fe4b368b7741889781d848",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chain = prompt_template | llm\n",
    "results = []\n",
    "\n",
    "for sample in tqdm(dataset):\n",
    "    scene = sample[\"scene\"]\n",
    "    question = sample[\"question\"]\n",
    "    result = chain.invoke(input={\n",
    "        \"scene\": scene,\n",
    "        \"question\": question\n",
    "    })\n",
    "\n",
    "    answer = None\n",
    "    while answer is None:\n",
    "        try:\n",
    "            answer = extract_json(result.content)[\"answer\"]\n",
    "        except json.JSONDecodeError:\n",
    "            print(\"JSON Decode Error\")\n",
    "            continue\n",
    "    results.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = [question[\"answer\"] for question in dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    \"predicted\": results,\n",
    "    \"actual\": answers\n",
    "})\n",
    "\n",
    "df.to_csv(f\"direct_prompt_results_{model_name}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 84 0.84\n"
     ]
    }
   ],
   "source": [
    "correct = [1 if str(p) == str(a) else 0 for p, a in zip(results, answers)]\n",
    "print(len(correct), sum(correct), sum(correct) / len(correct))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_objects = []\n",
    "for scene in scenes:\n",
    "    num_objects.append(len(scene[\"objects\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "intervals = [range(3, 6), range(6, 9), range(9, 11)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_questions(questions, scenes, objects_interval, count=20):\n",
    "    filtered_questions = []\n",
    "\n",
    "    for question in questions:\n",
    "        scene = scenes[question[\"image_index\"]]\n",
    "        num_objects = len(scene[\"objects\"])\n",
    "        \n",
    "        if num_objects in objects_interval:\n",
    "            filtered_questions.append(question)\n",
    "        \n",
    "        if len(filtered_questions) >= count:\n",
    "            break\n",
    "    \n",
    "    return filtered_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6065037d2dab4f818898b4aff5914617",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0168f09aae5e425783a38fb6f6980f24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "909d92f276c4446eb760f73a1afb48ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chain = prompt_template | llm\n",
    "results = []\n",
    "\n",
    "for i in range(3):\n",
    "    filtered_questions = get_questions(questions, scenes, intervals[i], count=20)\n",
    "    correct = 0\n",
    "    for question in tqdm(filtered_questions):\n",
    "        scene = scenes[question[\"image_index\"]]\n",
    "        process_scene(scene)\n",
    "        result = chain.invoke(input={\n",
    "            \"scene\": scene,\n",
    "            \"question\": question[\"question\"]\n",
    "        })\n",
    "\n",
    "        try:\n",
    "            answer = extract_json(result.content)[\"answer\"]\n",
    "        except json.JSONDecodeError:\n",
    "            continue\n",
    "        if  question[\"answer\"].lower() in answer.lower():\n",
    "            correct += 1\n",
    "    results.append(correct)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for range(3, 6) objects: 80.00%\n",
      "Accuracy for range(6, 9) objects: 85.00%\n",
      "Accuracy for range(9, 11) objects: 85.00%\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print(f\"Accuracy for {intervals[i]} objects: {results[i] / 20 * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_question_by_category(questions, category, count=20):\n",
    "    filtered_questions = []\n",
    "\n",
    "    for question in questions:\n",
    "        if category == \"count\" and question[\"answer\"].isdigit():\n",
    "            filtered_questions.append(question)\n",
    "        elif category == \"judge\" and question[\"answer\"].lower() in [\"yes\", \"no\"]:\n",
    "            filtered_questions.append(question)\n",
    "        elif category == \"query\" and question[\"answer\"].lower() not in [\"yes\", \"no\"] and not question[\"answer\"].isdigit():\n",
    "            filtered_questions.append(question)\n",
    "        \n",
    "        if len(filtered_questions) >= count:\n",
    "            break\n",
    "    \n",
    "    return filtered_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "486c6e65e0354c75a1291de155ff0e6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d5cd365ba524a068a2a5def6c0de064",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10ed1d21f80d4e938f645065a2be011a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chain = prompt_template | llm\n",
    "\n",
    "results = []\n",
    "answers = []\n",
    "for category in [\"count\", \"judge\", \"query\"]:\n",
    "    filtered_questions = get_question_by_category(questions, category, count=20)\n",
    "    correct = 0\n",
    "    ans = []\n",
    "    for question in tqdm(filtered_questions):\n",
    "        scene = scenes[question[\"image_index\"]]\n",
    "        process_scene(scene)\n",
    "        result = chain.invoke(input={\n",
    "            \"scene\": scene,\n",
    "            \"question\": question[\"question\"]\n",
    "        })\n",
    "\n",
    "        try:\n",
    "            answer = extract_json(result.content)[\"answer\"]\n",
    "        except json.JSONDecodeError:\n",
    "            continue\n",
    "        if  str(question[\"answer\"]).lower() in answer.lower():\n",
    "            correct += 1\n",
    "        ans.append((question[\"answer\"], answer))\n",
    "    answers.append(ans)\n",
    "    results.append(correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for count questions: 90.00%\n",
      "Accuracy for judge questions: 85.00%\n",
      "Accuracy for query questions: 70.00%\n"
     ]
    }
   ],
   "source": [
    "for i, category in enumerate([\"count\", \"judge\", \"query\"]):\n",
    "    print(f\"Accuracy for {category} questions: {results[i] / 20 * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 82 0.82\n"
     ]
    }
   ],
   "source": [
    "model_name = \"gpt-4o-mini\"\n",
    "df = pd.read_csv(f\"direct_prompt_results_{model_name}.csv\")\n",
    "\n",
    "print(len(df), sum(df[\"predicted\"] == df[\"actual\"]), sum(df[\"predicted\"] == df[\"actual\"]) / len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance of directly question answering using the JSON object\n",
    "* 4o-mini\n",
    "    * Clevr human: COT exact match: 13 / 25\n",
    "    * Clevr Val: 18 / 25\n",
    "* 4o\n",
    "    * Clevr human: COT exact match: 18 / 25\n",
    "    * Clevr Val: 21 / 25"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sherpa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
