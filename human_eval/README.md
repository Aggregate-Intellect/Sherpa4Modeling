# Sherpa for HumanEval (The Code Generation Use Case)
> [!NOTE]
>
> The following command assumes you are in the `human_eval` folder.

> [!Warning]
>
> While this use case can also be run using a virtual environment, it is highly recommended to run it in a Docker container or similar isolated environment because the experiments may **execute arbitrary code generated by LLMs**. Below we describe the steps using Jupyter lab in a docker container. Use the virtual environment only if you are aware of the risks and have taken necessary precautions.

## Organization
The use case is organized as follows:
* `run_programs.py`: The main script to run the code generation experiments
* `evaluation.ipynb`: The evaluation notebook to create tables and figures in the paper for the HumanEval use case
* `llm_coder` contains the code implementation for the use case. Specifically, it includes the implementation of the following approaches in the paper:
  * `agent_coder_improved` folder contains the implementation of the agent coder approach
  * `test_based_sm_with_feedback` folder contains the implementation of the test-based state machine approach
  * `coders/direct_prompt_coder.py` contains the implementation of the direct prompt approach
* `results` folder contains the cached results of the experiments run on the use case
* `tests` folder contains some tests for the code
* `.env_template` file contains the environment variables required to run the use case
* `requirements.txt` contains the requirements to run the use case
* `Dockerfile` and `docker-compose.yml` contains the Dockerfile to run the use case in a Docker container

## Installation Preparation
1. First, download the code for the human_eval benchmark from: https://github.com/openai/human-eval
```bash
git clone https://github.com/openai/human-eval
```
  * **NOTE:** Make sure you place the `human-eval` project under the same folder as this README file, i.e., the `human_eval` folder of the repository.
2. Then, copy the `human_eval` folder from this repository to the `human_eval` folder you just cloned.


## Installation with Docker (Recommended)
1. Install [Docker](https://docs.docker.com/get-started/overview/) if you haven't already. You can follow the instructions on the Docker website for your operating system.
2. Build the Docker image using the provided Dockerfile and docker compose (Note that the build context is the root directory of this repository, as specified by `..`).

```bash
docker compose build
docker compose up
```

This will start a jupyter lab server on port 8888 and information similar to the following will be printed in the terminal:
```bash
human_eval-jupyterlab-1  |     To access the server, open this file in a browser:
human_eval-jupyterlab-1  |         file:///root/.local/share/jupyter/runtime/jpserver-1-open.html
human_eval-jupyterlab-1  |     Or copy and paste one of these URLs:
human_eval-jupyterlab-1  |         http://45a69bbf717f:8888/lab?token=<token>
human_eval-jupyterlab-1  |         http://127.0.0.1:8888/lab?token=<token>
```

3. Open the Jupyter lab in your browser with `http://localhost:8888`, and provide the value of the `token` from the terminal output. then you will be able to run the experiments in the subsequent steps.

## Installation with Virtual Environment
> [!Warning]
> This use case may run arbitrary code generated by LLMs. It is highly recommended to run it in a Docker container or similar isolated environment. Use the virtual environment only if you are aware of the risks and have taken necessary precautions.

1. Create a new virtual environment for this use case (not required, but recommended)

   ```bash
   # For venv
   python -m venv humaneval

   # For conda
   conda create -n humaneval python=3.12
   ```

   Activate the virtual environment:

   ```bash
   # For venv
   source humaneval/bin/activate
   # For conda
   conda activate humaneval
   ```

2. Install the requirements:
   ```bash
   pip install -r requirements.txt
   ```

4. Install the `human_eval` benchmark:
   ```bash
    pip install -e human_eval
   ```


## Setup the Environment Variables
Create a `.env` file and copy the content of `.env_template` to it. Then, set the `OPENAI_API_KEY` and `TOGETHER_API_KEY` variables to your OpenAI and TogetherAI API keys, respectively.


## Run the Code Generation
First, open a new terminal in the Jupyter lab interface. Then, you can run the `run_programs.py` script to generate code using LLMs. This script contains the following arguments:
  * **-h,--help**: show this help message and exit
  * **--llm_family**: Provider of the model to use. One of {openai, togetherai}
  * **--llm_model**: Name of the LLM to use. The paper uses the following: gpt-4o (openai), gpt-4o-mini (openai), Qwen/Qwen2.5-7B-Instruct-Turbo (together), Qwen/Qwen2.5-7B-Instruct-Turbo (together) and Qwen/Qwen2.5-Coder-32B-Instruct (together). You can also use other LLMs from the two providers.
  * **--temperature**: Temperature of the LLM, default is 0.01
  * **--output_filename**: name of the output file to save the results
  * **--saving_frequency**: Frequency of saving the results to the output file (in terms of the number of programs generated), default is 30
  * **--num_parallel**: Number of parallel processes to use for generating the programs, default is 1
  * **--method**: Method to use for generating the programs. One of {direct_prompt, state_machine_with_feedback, agent_coder_improved}
    * **direct_prompt**: Directly use the LLM to generate the code
    * **state_machine_with_feedback**: The test-driven state machine approach to generate the code
    * **agent_coder_improved**: Use the agent coder approach-based state machine in the paper


For example, to run the direct prompt method with the gpt-4o-mini model, you can run the following command:
```bash
python run_programs.py --llm_family openai --llm_model gpt-4o-mini --temperature 0.01 --output_filename results/gpt-4o-mini/run1/direct_prompt.jsonl --method direct_prompt
```

Or to run the Qwen2.5-7B-Instruct-Turbo model with the state machine approach, you can run the following command:
```bash
python run_programs.py --llm_family togetherai --llm_model Qwen/Qwen2.5-7B-Instruct-Turbo --temperature 0.01 --output_filename results/Qwen2.5-7B-Instruct-Turbo/run1/state_machine_with_feedback.jsonl --method state_machine_with_feedback
```

To repeat the experiments in the paper, run each LLM three times with the same command, but change the `--output_filename` argument to save the results in a different file.

> [!Note]
>
> The command caches results from the LLM in the frequency specified by the `--saving_frequency` argument. If you run the same command again, it will continue from where it left off and will not overwrite the previous results. You can change the `--output_filename` argument to save the results in a different file.

The `results` folder contains the cached 3-run results of the approaches used in the paper.

The result will generate a `jsonl` file with the generated code and the number of LLM calls made to generate the code. Each line in the file will contain a JSON object.

### Evaluation
#### With Docker
The evaluation steps are included in the `evaluation.ipynb` notebook to create tables and figures in the paper for the HumanEval use case. You can open the notebook in Jupyter lab and execute all the cells to generate the tables and figures.

#### With virtural environment
```bash
jupyter notebook evaluation.ipynb
```

Execute all the cells in the notebook to generate the tables and figures. 