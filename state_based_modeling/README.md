# Sherpa for Modeling (The Class Name Generation Use Case)
Designing state machines for solving the class name generation task in the Modeling dataset

## Organization
The use case is organized as follows:
* `evaluation` contains the code implementation for the evaluation of the use case
* `modeling` contains the code implementation for the use case
  * The file `model_class.py` contains the implementation of the Inspect state machine
  * The file `model_class_mig.py` contains the implementation of the MIG state machine
* `ground_truth` contains the ground truth model for the dataset
* `results` contains the cached results of the experiments run on the use case
* `scripts` contains the scripts to run the class name generation task
    * The file `direct_main.py` contains the implementation of the direct approach
* `.env_template` file contains the environment variables required to run the use case.
* `requirements.txt` contains the requirements to run the use case
* `requirements_eval.txt` contains the requirements to run the evaluation notebook
* `evaluation.ipynb` contains the evaluation notebook to create tables and figures in the paper for the Modeling use case
* `modeling_problems.csv` contains samples from the Modeling dataset

## Installation
1. Create a new virtual environment for this use case (not required, but recommended)

   ```bash
   # For venv
   python -m venv modeling

   # For conda
   conda create -n modeling python=3.12
   ```

   Activate the virtual environment:

   ```bash
   # For venv
   source modeling/bin/activate
   # For conda
   conda activate modeling
   ```

2. Install `sherpa` following the top-level Read
3. Install the requirements:
   ```bash
   pip install -r requirements.txt
   ```

## Setup the Environment Variables
Create a `.env` file and copy the content of `.env_template` to it. Then, set the `OPENAI_API_KEY` and `TOGETHER_API_KEY` variables to your OpenAI and TogetherAI API keys, respectively.

## Run Class Name Generation
### Run the Direct Approach
Run `scripts/direct_main.py` to generate class names using LLMs only. It contains the following commands

* **-h, --help**: show this help message and exit
* **--model_type**: Provider of the model to use. One of {openai, together}
* **--llm**: Name of the LLM to use. The paper uses the following: gpt-4o (openai), gpt-4o-mini (openai), Qwen/Qwen2.5-7B-Instruct-Turbo (together), Qwen/Qwen2.5-7B-Instruct-Turbo (together) and Meta-Llama-3.1-70B-Instruct-Turbo (together). You can also use other LLMs from the two providers.
* **--run_number**: Run number to use for the experiment. This is used to create a unique output folder for the results.
* **--output_folder**: Output folder to save the results, default is `results_new`
* **--num_processes**: Number of processes to use for parallel processing of the dataset, default is 1

For example, to run the direct approach with the gpt-4o-mini model, you can run the following command:

```bash
python -m scripts.direct_main --model_type=openai --llm=gpt-4o-mini --run_number 1 --num_processes 1
```

To repeat the experiments in the paper, run each LLM three times with the same command, but change the `--run_number` argument to save the results in a different folder.

The command will output a `txt` file for each class name problem, containing the class name generated by the LLM. 

### Run the State Machine Approaches
Run `scripts.sm_main.py` to generate class names using the state machine approaches. It contains the following commands:
* **-h, --help**: show this help message and exit
* **--model_type**: Provider of the model to use. One of {openai, together}
* **--llm**: Name of the LLM to use. The paper uses the following: gpt-4o (openai), gpt-4o-mini (openai), Qwen/Qwen2.5-7B-Instruct-Turbo (together), Qwen/Qwen2.5-7B-Instruct-Turbo (together) and Meta-Llama-3.
* **--run_number**: Run number to use for the experiment. This is used to create a unique output folder for the results.
* **--output_folder**: Output folder to save the results, default is `results_new`
* **--num_processes**: Number of processes to use for parallel processing of the dataset, default is 1
* **--approach**: Approach to use for generating class names. One of {sherpa, sherpa_mig}
    * Sherpa runs the Inspection State machine in the paper
    * sherpa_mig runs the MIG state machine in the paper


For example, to run the Inspection state machine with the gpt-4o-mini model, you can run the following command:
```bash
python -m scripts.sm_main --model_type=openai --llm=gpt-4o-mini --run_number 1 --approach sherpa --num_processes 1
```

Or running the MIG state machine with the Meta-Llama-3.1-70B-Instruct-Turbo model, you can run the following command:
```bash
python -m scripts.sm_main --model_type=together --llm=meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo --run_number 1 --approach sherpa_mig --num_processes 1
```

To repeat the experiments in the paper, run each LLM three times with the same command, but change the `--run_number` argument to save the results in a different folder.

The command will output a `txt` file for each class name problem, containing the class name generated by the LLM. It will also output a `json` file for each class name problem, containing the number of LLM calls made to generate the class names.

## Evaluation
Some dependencies in the evaluation part of this use case requires Python 3.8, so it is recommended to create a separate virtual environment for the evaluation part.

```bash
# For venv
python -m venv modeling_evaluation

# For conda
conda create -n modeling_evaluation python=3.8
```

Activate the virtual environment:

```bash
# For venv
source modeling_evaluation/bin/activate
# For conda
conda activate modeling_evaluation
```

Then, install the requirements for the evaluation part:
```bash
pip install -r requirements_eval.txt
```

Then execute the `evaluation.ipynb` notebook to create tables and figures in the paper for the Modeling use case. To run the notebook, run the following command:

```bash
jupyter notebook evaluation.ipynb
```

> ![NOTE]
> 
> Note that the evaluation approach uses OpenAI ada embedding to embed the model contents for evaluation. the embedding results are cached in the `results` folder for cost and time efficiency. If you want to re-run the evaluation, you can delete all the `*_ada_embedding.txt` file in the results folder.